# -*- coding: utf-8 -*-
"""idx_credit_risk.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UOMCzvXtNEpNPOfqoLhYZxiVfKVe3irZ

# Credit Risk Model Prediction ID/X Partners
Author: Rizki Dinata (June 2024)
"""

from google.colab import drive
drive.mount('/content/drive/')

"""# Import library and dataset"""

import numpy as np
import pandas as pd

pd.set_option('display.max_columns', None)

import matplotlib.pyplot as plt
import seaborn as sns

import warnings
warnings.filterwarnings('ignore')
import os

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_val_score

from sklearn.metrics import roc_curve, accuracy_score, confusion_matrix, classification_report, roc_auc_score, recall_score, precision_score, f1_score
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import OneHotEncoder
from imblearn.over_sampling import SMOTE

from scipy.stats import chi2_contingency, f_oneway

os.chdir('/content/drive/MyDrive/Projects/credit-risk-idx')

!pwd

#load data
df = pd.read_csv('idx_credit.csv')
df.head()

#display total rows and columns
df.shape

df.info()

#split categorical and numerical data
cat_df = df.select_dtypes(include='object')
num_df = df.select_dtypes(exclude='object')

cats_col = cat_df.columns.to_list()
nums_col = num_df.columns.to_list()

df[nums_col].describe()

#display count unique value categorical data
for col in cats_col:
  print(df[col].value_counts())
  print('---'*30)

"""# Data Preprocessing"""

df_clean = df.copy()

"""## Drop Columns with Missing Values"""

#check missing value with percentage and add style background

def missing_percentage(df):
  missing_values = df.isnull().sum()
  total_values = len(df)
  missing_percentage = (missing_values / total_values) * 100
  missing_df = pd.DataFrame({'column_name': df.columns,
                             'missing_values': missing_values,
                             'missing_percentage': missing_percentage
                              })
  missing_df = missing_df.sort_values('missing_percentage', ascending=False).reset_index(drop=True)
  missing_df['missing_percentage'] = missing_df['missing_percentage'].apply('{:.2f}'.format)
  return missing_df

result_missing = missing_percentage(df_clean)
result_missing.style.background_gradient(cmap='Reds', subset=['missing_percentage'])

#drop feature or columns with missing value percentage more than 50

df_clean = df_clean.drop(columns=result_missing[result_missing['missing_percentage'].astype('float') > 50]['column_name'])

df_clean.shape

"""## Impute Missing Values under 50%"""

# define impute missing value if number input median value and if its object impute mode

def impute_missing_values(df):
  for col in df.columns:
    if df[col].dtype == 'number':
      df[col] = df[col].fillna(df[col].median())
    else:
      df[col] = df[col].fillna(df[col].mode()[0])
  return df

df_clean = impute_missing_values(df_clean)

df_clean.info()

"""## Check Duplicate data"""

df_clean.duplicated().sum()

"""## Check unique values"""

print("Unique Values Numerical: \n")
df_clean.select_dtypes(include='number').nunique()

"""* `Unnamed: 0 `,`id`, `member_id` memiliki nilai yang berbeda pada tiap baris nya
* Sedangkan `policy_code` hanya memiliki 1 nilai unik saja pada keseluruhan data
"""

print("Unique Values Categorical: \n")
df_clean.select_dtypes(include='object').nunique()

"""* `emp_title`, `url`, `title` memiliki nilai unik yang banyak yaitu diatas 1000, dimana nilai ini sangat banyak untuk fitur yang bersifat kategorikal.
* `application_type` hanya memiliki 1 nilai unik
* `issue_d`, `last_pymnt_d`, `next_pymnt_d`, `last_credit_pull_d` merupakan fitur yang seharusnya bertipe **datetime** tetapi terdeteksi object pada database. Oleh karena itu, nanti nya fitur ini perlu diubah type data nya.
"""

df_clean['term'].unique()

"""* Kita perlu menghapus whitespace pada fitur"""

# delete whitespcae unique value in feature term

df_clean['term'] = df_clean['term'].apply(lambda x: x.strip())
df_clean['term'].unique()

df_clean['grade'].unique()

df_clean['sub_grade'].unique()

df_clean['emp_length'].unique()

df_clean['home_ownership'].unique()

df_clean['verification_status'].unique()

df_clean['pymnt_plan'].unique()

"""## Define target variable

* `loan_status` akan menjadi target variabel pada proses kali ini
* Dikarenakan fitur ini memiliki 9 nilai unik, sehingga kita perlu regrup menjadi binary classification (2 classes)
"""

df_clean['loan_status'].unique()

df_clean['loan_status'].value_counts()

"""* Bad loan (1) : `Charged Off`, `Default`, `Late (31-120 days)`, `Does not meet the credit policy. Status:Charged Off`, `Late (16-30 days)`
* Good loan (0) : `Fully Paid`, `Does not meet the credit policy. Status:Fully Paid`
* Uncertained loan (-1) : `Current`, `In Grace Period`
* Kita akan menggunakan Good loan (1) dan Bad loan (0) sebagai target binary classification
* Uncertained loan (-1) akan di hapus dari dataframe karena sedang proses masa pinjaman yang tidak dapat di klasifikasikan sebagai good atau bad.
"""

# Define dictionary for encoding target variable
target = {'Fully Paid':'Good loan',
          'Does not meet the credit policy. Status:Fully Paid':'Good loan',
          'Charged Off':'Bad loan',
          'Default':'Bad loan',
          'Late (31-120 days)':'Bad loan',
          'Does not meet the credit policy. Status:Charged Off':'Bad loan',
          'Late (16-30 days)':'Bad loan',
          'Current':'Uncertained loan',
          'In Grace Period':'Uncertained loan'}
# Mapping values in target column
df_clean['loan_status'] = df_clean['loan_status'].map(target)

df_clean['loan_status'].value_counts()

# drop loan status having value -1

df_clean = df_clean[df_clean['loan_status'] != 'Uncertained loan'].reset_index()
df_clean['loan_status'].value_counts()

df_clean.shape

"""## Reformatting Datetime columns"""

df_clean['earliest_cr_line'].value_counts()

def date(dt):
  if dt.year > 2016:
    dt = dt.replace(year=dt.year - 100)
  return dt

# The month which the loan was funded
df_clean['issue_d'] = pd.to_datetime(df_clean['issue_d'], format='%b-%y')
df_clean['last_pymnt_d'] = pd.to_datetime(df_clean['last_pymnt_d'], format='%b-%y')
df_clean['last_credit_pull_d'] = pd.to_datetime(df_clean['last_credit_pull_d'], format='%b-%y')
df_clean['earliest_cr_line'] = pd.to_datetime(df_clean['next_pymnt_d'], format='%b-%y')
df_clean['earliest_cr_line'] = df_clean['earliest_cr_line'].apply(lambda x: date(x))

# display result
df_clean[['issue_d', 'last_pymnt_d', 'last_credit_pull_d', 'earliest_cr_line']].tail()

"""## Create new features

Pada proses kali ini kita akan membuat fitur baru:

* `credit_pull_year` = total tahun antara paling awal peminjam membuka credit (`earliest_cr_line`) dan terakhir menarik pinjaman kredit (`last_credit_pull_d`)
* `pyment_time` = total bulan antara pinjaman didanai (`issue_d`) dan terakhir membayar peminjaman (`last_pymnt_d`)
"""

# define function to create new features
def total_month(dt1, dt2):
  return (dt2.year - dt1.year) * 12 + (dt2.month - dt1.month)

def total_year(dt1, dt2):
  return dt2.year - dt1.year

df_clean['credit_pull_year'] = df_clean.apply(lambda x: total_year(x['last_credit_pull_d'], x['earliest_cr_line']), axis=1)
df_clean['pyment_time'] = df_clean.apply(lambda x: total_month(x['issue_d'], x['last_pymnt_d']), axis=1)

# display result
df_clean[['credit_pull_year', 'pyment_time']].head()

df_clean.info()

df_clean.head()

"""## Save Cleaned Data"""

# save cleaned data
df_clean.to_csv('idx_credit_clean.csv', index=False)
print('Saving data is done!!')

"""**Data ini akan digunakan untuk Exploratory Data Analysis (EDA) untuk menemukan informasi tersembunyi dan bermanfaat**

# Exploratory Data Analysis (EDA)

## Target Variable
Loan status merupakan variabel target yang digunakan pada proses kali ini.
"""

# visualize loan status before transforming to binary classification
orders = df['loan_status'].value_counts().index

colors = ["dodgerblue" if stats in ['Fully Paid', 'Does not meet the credit policy. Status:Fully Paid'] else
          "red" if stats in ['Charged Off', 'Default', 'Late (31-120 days)', 'Does not meet the credit policy. Status:Charged Off', 'Late (16-30 days)'] else
          "gray" for stats in orders]

plt.figure(figsize=(10, 6))

sns.countplot(y='loan_status', order=orders, palette=colors,  data=df)
plt.title('Loan Status Distribution', fontsize=16)
plt.xlabel('Count', fontsize=12)
plt.ylabel('Loan Status', fontsize=12)


plt.show()

"""Current dan In Grace Period akan di drop karena tidak bisa ditentukan sebagai Good loan atau Bad loan.

## Load cleaned data from storage
"""

# load cleaned data
df_clean = pd.read_csv('idx_credit_clean.csv')
df_clean.head()

# Calculate profit and loss for good and bad loans
good_loans = df_clean[df_clean['loan_status'] == 'Good loan']
bad_loans = df_clean[df_clean['loan_status'] == 'Bad loan']

good_loan_profit = good_loans['total_pymnt'].sum() - good_loans['total_rec_prncp'].sum()
bad_loan_loss = bad_loans['total_rec_prncp'].sum() - bad_loans['total_pymnt'].sum()

print(f"Profit from good loans: {good_loan_profit}")
print(f"Loss from bad loans: {bad_loan_loss} \n")

# create pie chart
plt.figure(figsize=(15, 4))

plt.subplot(1,2,1)
df_clean['loan_status'].value_counts().plot(kind='pie',
                                            autopct='%1.1f%%',
                                            startangle=90,
                                            explode=[0,0.05],
                                            colors=['dodgerblue','red'])

# create bar plot
plt.subplot(1,2,2)
sns.countplot(x='loan_status', data=df_clean, palette=['dodgerblue','red'])

plt.show()

"""* Berdasarkan distribusi loan status, terdapat imbalanced data dimana **78.2%** Good loan sedangkan **21.8%** Bad loan.
* Diperkirakan kentungan yang diperoleh dari Good loan yaitu **382,698,243** dan total kerugian dari Bad loan yaitu **-166,701,734**

## Plotting Function
"""

# 1. Define Barplot
def barplot_cust(data, x, y, palette=None, hue=None, order=None):
  plt.figure(figsize=(10, 6))
  sns.barplot(x=x, y=y, data=data, palette=palette, hue=hue, order=order)

  plt.title(f"{hue.replace('_',' ').title()} {y.replace('_',' ').title()} by {x.replace('_',' ').title()}")
  plt.xlabel(f"{x.replace('_',' ').title()}")
  plt.ylabel(f"{y.replace('_',' ').title()}")
  plt.legend(f"{hue.replace('_',' ').title()}")


# 2. Define Line plot
def lineplot_cust(data, x, y, hue=None, palette=None, ci=95):
  sns.lineplot(x=x, y=y, data=data, hue=hue, palette=palette, ci=ci)

  plt.title(f"{hue.replace('_',' ').title()} {y.replace('_',' ').title()} by {x.replace('_',' ').title()}")
  plt.xlabel(f"{x.replace('_',' ').title()}")
  plt.ylabel(f"{y.replace('_',' ').title()}")
  plt.legend(title=f"{hue.replace('_',' ').title()}", loc='best')

"""## Loan over years"""

# change issue_d to datetime
df_clean['issue_d'] = pd.to_datetime(df_clean['issue_d'])

# create new column year
df_clean['year'] = df_clean['issue_d'].dt.year

plt.figure(figsize=(18, 8))

# Funded amnt by years
plt.subplot(2,2,1)
lineplot_cust(x='year', y='funded_amnt', data=df_clean, hue='loan_status', palette=['dodgerblue','red'])
plt.xlabel(None)

# Debt to income ratio by years
plt.subplot(2,2,3)
lineplot_cust(x='year', y='dti', data=df_clean, hue='loan_status', palette=['dodgerblue','red'])
plt.xlabel(None)

# annual income by years
plt.subplot(2,2,2)
lineplot_cust(x='year', y='annual_inc', data=df_clean, hue='loan_status', palette=['dodgerblue','red'])
plt.xlabel(None)

# Revolving balance by years
plt.subplot(2,2,4)
lineplot_cust(x='year', y='revol_bal', data=df_clean, hue='loan_status', palette=['dodgerblue','red'])
plt.xlabel(None)

plt.tight_layout()
plt.show()

"""* Funded Amount over years
  - Total pinjaman yang diberikan mengalami kenaikan dari tahun tahun ke tahun.
  - Total pinjaman yang diberikan pada Good loan lebih sedikit daripada pinjaman yang diberikan pada Bad loan
  - Pada tahun 2009 hingga 2010 pinjaman yang diberikan mengalami penurunan pada Bad loan tetapi pada tahun berikutnya mengalami peningkatan lagi

* Annual Income over years
  - Fitur ini menjelaskan pendapatan tahunan yang dilaporkan langsung dari klien.
  - Terdapat hal menarik disini dimana pendapatan Good loan lebih rendah dari Bad loan pada tahun 2007 hingga 2008. Tetapi pada tahun setelahnya pendapatan Good loan terus mengalami peningkatan.
  - Sedangkan pendapatan Bad loan walaupun lebih tinggi pada tahun 2007 - 2008 dari pendapatan Good loan, tetapi terus mengalami penurunan hingga memiliki selisih hingga 10,000 pada data terakhir.

* Debt to Income (DTI) over years
  - DTI merupakan rasio antara debt dan pendapatan
  - DTI terus mengalami peningkatan dari tahun ke tahun
  - DTI Bad loan lebih tinggi dari DTI Good loan.
  - Data ini menjelaskan bahwa pendapatan dari klien mengalami penurunan dan jumlah pinjaman yang yang terus peningkatan oleh klien Bad loan. Tetapi menurut beberapa sumber, DTI ini masih bisa di toleransi karena masih di bawa 36.

* Revolving Balance over years
  - Terjadi lonjakan tinggi Revol bal Bad loan pada tahun 2008 dan juga lebih tinggi dari Good loan sekitar 10,000. Ini juga menjelaskan dimana pendapatan tahunan(annual income) Bad loan masih tinggi pada tahun yang sama.
  - Kemudian pada tahun selanjutnya, revol bal Bad loan terus mengalami penurunan hingga mengikuti tren Revol bal Good loan.
  - Hal ini mengindikasikan bahwa klien berusaha mengurangi total Revol bal karena pendapatan yang menurun sehingga finansial nya tidak stabil.

* Dari data ini kita mengetahui bahwa klien Bad loan mengalami penurunan pendapatan tetapi kebutuhan mereka tetap naik. Kemudian klien Bad loan ingin mengurangi total hutang mereka secara perlahan dan juga akibat total hutang mereka yang bengkak pada tahun 2008.   
* Sedangkan klien Good loan walaupun pendapatan mereka terus naik, tapi jumlah revol balance mereka menyesuaikan dengan pendapatan mereka. Dimana bisa dlihat DTI mereka yang tidak ada lonjakan walaupun tren peningkatan terus terjadi. Tren peningkatan DTI ini hampir mengikuti tren pendapatan klien Good loan itu sendiri.

## Loan status by purpose
"""

# define function custom barplot
def barplot_cust(df, y, palette=None, num_top=5):
    cnt = df[y].value_counts()

    # Plot the countplot
    num_top = num_top
    palette = ['dodgerblue']*num_top + ['gray']*(len(cnt) - num_top)

    ax = sns.countplot(y=y, data=df, order=cnt.index, palette=palette)

    # Add the count labels to the bars

    for p in ax.patches:
        # Calculate label position
        width = p.get_width()
        x = width + 0.1  # Adjust offset as needed

        # Add the label
        ax.text(x, p.get_y() + p.get_height() / 2,
                '{:1.0f}'.format(width),
                ha="left", va="center", fontsize='small')
    plt.title(f"Distribution of {y.title()}")
    plt.xlabel('Count')
    plt.ylabel(y.title())


# define function stacked plot
def stacked_barplot(df, x, y, palette=None, order=None):
    # grouping by variable and loan status
    var = df.groupby(x)[y].value_counts().unstack()
    var = var.reindex(order)

    # calculate percentage
    var_pct = var.div(var.sum(axis=1), axis=0).mul(100)

    # Determine the order of the bars for stacking
    bar1 = plt.bar(var_pct.index,
                   var_pct['Good loan'],
                   label='Good loan',
                   color=palette[0])
    bar2 = plt.bar(var_pct.index,
                   var_pct['Bad loan'],
                   label='Bad loan',
                   color=palette[1],
                   bottom=var_pct['Good loan'])

    # Set the order of the stacked bars
    stack_order = ['Bad loan', 'Good loan']

    # Add labels to each bar segment
    for bar1, bar2 in zip(bar1, bar2):
        height1 = bar1.get_height()
        height2 = bar2.get_height()

        plt.text(bar1.get_x() + bar1.get_width() / 2,
                 height1 / 2,
                 '{:.1f}%'.format(height1),
                 ha='center',
                 va='center')

        plt.text(bar2.get_x() + bar2.get_width() / 2,
                 height1 + height2 / 2,
                 '{:.1f}%'.format(height2),
                 ha='center',
                 va='center')

    # Add label and set the order of the stacked bars
    plt.title(f"Default Rate by {x.replace('_', ' ').title()}", fontsize='large')
    plt.xlabel(f"{x.replace('_', ' ').title()}")
    plt.ylabel('Total Clients (%)')
    plt.legend(title=y.replace('_', ' ').title(), loc='best', bbox_to_anchor=(1.01, 1))

# select top 5 purpose of loan
top_val = df_clean['purpose'].value_counts().head(5).index
top_purpose = df_clean[df_clean['purpose'].isin(top_val)]
order_purpose = top_purpose['purpose'].value_counts().index

# create figure
plt.figure(figsize=(18, 6))

# plot all purpose
plt.subplot(1,2,1)
barplot_cust(df=df_clean, y='purpose')

# plot percentage top 5 purpose
plt.subplot(1,2,2)
stacked_barplot(df=top_purpose, x='purpose', y='loan_status', order=order_purpose, palette=['dodgerblue','red'])

plt.tight_layout()
plt.show()

"""* Insight
  - Terdapat 5 kepentingan yang dilakukan klien untuk melakukan peminjaman yaitu debt consolidation, credit card, home improvement, other(lain-lain), dan major purchase
  - Major purchase memiliki tingkat Good loan yang tinggi yaitu 83.9%. Sedangkan tingkat Bad loan yang tinggi yaitu Other(kepentingan lain-lain)
  - Dikarenakan kepentingan tidak bisa kita tentukan secara spesifik, sehingga kita memilih tingkat Bad loan tertinggi setelah Other yaitu Debt consolidation yaitu 22.9%
  - Sehingga Debt Consolidation layak menjadi titik perhatian kita dikarenakan Debt consolidation juga merupakan kepentingan klien paling banyak untuk melakukan peminjaman yaitu sekitar 139,000

## Loan status by state
"""

# select top 5 states
top_state = df_clean['addr_state'].value_counts().head(5).index
top_state_df = df_clean[df_clean['addr_state'].isin(top_state)]
order_state = top_state_df['addr_state'].value_counts().index

plt.figure(figsize=(18, 6))

# plot by addr state
plt.subplot(1,2,1)
barplot_cust(df=top_state_df, y='addr_state')

# plot percentage by addr state
plt.subplot(1,2,2)
stacked_barplot(df=top_state_df, x='addr_state', y='loan_status', order=order_state, palette=['dodgerblue','red'])

plt.tight_layout()
plt.show()

"""* Insight
  - Terdapat 5 state tertinggi dimana lokasi asal klien yaitu CA (California), NY (New York), TX (Texas), FL (Florida), dan NJ (New Jersey).
  - Klien terbanyak berasal dari California yaitu **40550**
  - Tingkat rate Good loan tertinggi yaitu klien dari TX (Texas) yaitu **80.2%**
  - Sedangkan tingkat rate Bad loan tertinggi yaitu klien dari FL (Florida) yaitu **24.3%**

## Loan status by verification status
"""

# loan status by verification status


plt.figure(figsize=(18, 6))

plt.subplot(1,2,1)
barplot_cust(df=df_clean, y='verification_status')

plt.subplot(1,2,2)
stacked_barplot(df=df_clean, x='verification_status', y='loan_status', palette=['dodgerblue','red'])

"""* Insight
  - Klien yang pendapatan terverifikasi memiliki jumlah terbanyak yaitu **88,823**
  - Tetapi klien yang pendapatan terverifikasi pula memiliki tingkat rate Bad loan yang tertinggi yaitu **24.6%**
  - Sedangkan pendapatan (income) klien yang tak terverifikasi justru memiliki tingkat rate Bad loan yang paling rendah yaitu **17.1%**

## Loan status by Grade
"""

# loan status by grade
plt.figure(figsize=(18, 6))

plt.subplot(1,2,1)
barplot_cust(df=df_clean, y='grade')

plt.subplot(1,2,2)
stacked_barplot(df=df_clean, x='grade', y='loan_status', palette=['dodgerblue','red'])

"""* Insight
  - Klien dengan Grade B memiliki jumlah terbanyak yaitu **72,239**
  - Tingkat rate Bad loan tertinggi yaitu Grade G sekitar **47.5%**
  - Sedangkan tingkat rate Bad loan terendah yaitu Grade A sekitar **7.5%**
  - Hal ini menjelaskan bahwa tingkatan Grade klien semakin tinggi maka tingkat rate Bad loan semakin rendah.

## Loan status by employee length
"""

# loan status by employee length
plt.figure(figsize=(25, 6))

plt.subplot(1,2,1)
barplot_cust(df=df_clean, y='emp_length')

plt.subplot(1,2,2)
stacked_barplot(df=df_clean, x='emp_length', y='loan_status', palette=['dodgerblue','red'])

"""* Insight
  - Klien dengan lama masa kerja 10 tahun keatas memiliki jumlah klien terbanyak yaitu 80,537
  - Sedangkan tingkat rate Bad loan tidak jauh berbeda sekitar 21%

## Loan status by term
"""

# loan status by term
plt.figure(figsize=(20,4))

plt.subplot(1,2,1)
barplot_cust(df=df_clean, y='term')

plt.subplot(1,2,2)
stacked_barplot(df=df_clean, x='term', y='loan_status', palette=['dodgerblue','red'])

"""* Insight
  - Term 36 months memiliki jumlah klien tertinggi yaitu 186,469 atau lebih dari 60% klien memilih term 36 months (3 years)
  - Tingkat rate Bad loan tertinggi yaitu dengan term 60 months sekitar 35.5%.
  - Hal ini cukup menjelaskan bahwa semakin lama term yang ditentukan maka tingkat rate Bad loan pula semakin tinggi.

# Drop unnecessary Columns
"""

un_col = [
    'index',
    'id',
    'member_id',
    'Unnamed: 0',
    'url',
    'title',
    'zip_code',
    'addr_state',
    'sub_grade',
    'emp_title',
    'pymnt_plan',
    'collections_12_mths_ex_med',
    'policy_code',
    'application_type',
    'out_prncp',
    'out_prncp_inv',
    'total_rec_prncp',
    'total_rec_late_fee',
    'recoveries',
    'collection_recovery_fee'
    ]

col_date = [
    'issue_d',
    'last_pymnt_d',
    'last_credit_pull_d',
    'earliest_cr_line',
    'next_pymnt_d'
    ]

remove_col = un_col + col_date

df_cl = df_clean.copy()
df_cl = df_cl.drop(columns=remove_col, axis=1)

"""* Fitur `url`, `title`, `addr_state`, `sub_grade`, `emp_title` akan di drop bersamaan dengan kolom yang tidak diperlukan untuk modelling karena jumlah kategori yang terlalu banyak.
* Fitur datetime pula akan di drop pula karena jumlah nilai unik yang sedikit.

# Train Test Split
"""

X = df_cl.drop(columns=['loan_status'])
y = df_cl[['loan_status']]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"y_test shape: {y_test.shape}")

"""# Feature Engineering

## Encoding Data

One Hot Encoding merupakan salah satu metode untuk mengkonversikan kolom yang bersifat kategori menjadi binary untuk mempermudah model machine learning dalam menganalisis dan memprediksi. One Hot Encoding merupakan library encodde yang telah di sediakan oleh Scikit-learn.\
Selain itu, encoding dapat juga dilakukan secara manual dengan membuat dictionary target encoding.
"""

# defince function encoding data
def encoding_x(df):
  # Map grade
  grade_dict = {'A':6,
                'B':5,
                'C':4,
                'D':3,
                'E':2,
                'F':1,
                'G':0}
  df['grade'] = df['grade'].map(grade_dict)

  # map emp_length
  emp_length_dict = {'< 1 year':0,
                    '1 year':1,
                    '2 years':2,
                    '3 years':3,
                    '4 years':4,
                    '5 years':5,
                    '6 years':6,
                    '7 years':7,
                    '8 years':8,
                    '9 years':9,
                    '10+ years':10}
  df['emp_length'] = df['emp_length'].map(emp_length_dict)

  # map term
  term_dict = {'36 months':36,
               '60 months':60}
  df['term'] = df['term'].map(term_dict)

  # map home ownership and encoding with OHE -----------
  home_dict = {'MORTGAGE':'MORTGAGE',
               'RENT':'RENT',
               'OWN':'OWN',
               'OTHER':'OTHER',
               'ANY':'OTHER',
               'NONE':'OTHER'}
  df['home_ownership'] = df['home_ownership'].map(home_dict)

  # aggregate variable purpose and encoding eith OHE --------------
  purpose_dict = {'debt_consolidation':'debt_consolidation',
               'credit_card':'credit_card',
               'home_improvement':'private_use',
               'other':'other',
               'major_purchase':'major_purchase',
               'small_business':'small_business',
               'car':'private_use',
               'medical':'private_use',
               'wedding':'private_use',
               'moving':'private_use',
               'house':'private_use',
               'vacation':'private_use',
               'educational':'private_use',
               'renewable_energy':'other'}
  df['purpose'] = df['purpose'].map(purpose_dict)

  # Encoding with OHE
  var_ohe = ['home_ownership', 'verification_status', 'purpose', 'initial_list_status']
  encoder = OneHotEncoder(sparse=False,handle_unknown='ignore')

  for var in var_ohe:
    # Fit and transform columns
    encoded_var = encoder.fit_transform(df[[var]])

    # Get new columns
    encoded_var_columns = encoder.get_feature_names_out([var])

    # Create new dataframe encoded columns
    encoded_df = pd.DataFrame(encoded_var, columns=encoded_var_columns, index=df.index)

    # merge real data with dataframe encoded
    df = pd.concat([df, encoded_df], axis=1)

    # Drop columns after encoded
    df.drop(var, axis=1, inplace=True)
  print('Encoding features success!')
  return df

def encoding_y(df):
    # map target
    lstatus = {'Good loan':0, 'Bad loan':1}
    df['loan_status'] = df['loan_status'].map(lstatus)

    print('Encoding target success!')
    return df

# Apply to data train
X_train = encoding_x(X_train)
y_train = encoding_y(y_train)

# Apply to data test
X_test = encoding_x(X_test)
y_test = encoding_y(y_test)

"""# Feature Selection

## Pearson test
"""

# correlation matrix
var_corr = X_train.corr()

plt.figure(figsize=(30,13))
sns.heatmap(var_corr, annot=True, cmap='coolwarm')
plt.show()

# drop multicolinearity columns
multi_cols = ['loan_amnt','funded_amnt_inv','installment','total_pymnt','total_pymnt_inv','total_rec_int','int_rate','total_rev_hi_lim']

# apply to data
X_train = X_train.drop(columns=multi_cols, axis=1)
X_test = X_test.drop(columns=multi_cols, axis=1)

"""Drop kolom multikolinearitas untuk mencegah kesalahan model dalam memprediksi koefisien hubungan antara variabel independen dan variabel dependen

## Detect outliers
"""

def detect_outliers(df, cols):
  for col in cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1

    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # outlier check
    df[col] = np.where((df[col] < lower_bound) | (df[col] > upper_bound), np.clip(df[col],lower_bound,upper_bound), df[col])

  return df

def box_outliers(df, cols, width=18, height=10):
  rows = (len(cols) - 1) // 9 + 1
  plt.figure(figsize=(width, height))

  for i, col in enumerate(cols, 1):
    plt.subplot(rows, 9, i)
    sns.boxplot(df[col], orient='v')
    plt.ylabel(None)
    plt.title(f"{col.title()}", fontsize='medium')
    plt.tight_layout()

  plt.show()


def outliers_kde(df, cols, width=18, height=10):
    num_rows = (len(cols) - 1) // 9 + 1

    plt.figure(figsize=(width, height))

    for i, col in enumerate(cols,1):
        plt.subplot(num_rows, 9, i)
        sns.kdeplot(df[col], fill=True, alpha=0.7)
        plt.title(col, fontsize="medium")
        plt.xlabel(None)
        plt.ylabel(None)
        plt.tight_layout()

    plt.show()

num_cols = X_train.select_dtypes(include='number').columns

# check box outliers
box_outliers(X_train, num_cols)

# check with kde
outliers_kde(X_train, num_cols)

# focus remove outliers on the features not new features after encoding
cols_target = ['annual_inc','delinq_2yrs','inq_last_6mths','open_acc','pub_rec','revol_bal','revol_util','tot_cur_bal','credit_pull_year','pyment_time','last_pymnt_amnt','acc_now_delinq','tot_coll_amt','total_acc']


X_train = detect_outliers(X_train, cols_target)

# check box outliers
box_outliers(X_train, num_cols)

"""Kita akan drop fitur yang bernilai 0"""

cols_drop = ['pub_rec', 'delinq_2yrs','acc_now_delinq', 'tot_coll_amt']

X_train = X_train.drop(columns=cols_drop, axis=1)
X_test = X_test.drop(columns=cols_drop, axis=1)

"""# Modelling

## Scaling Data (Standard scaler)
Standard scaler digunakan untuk transformasi skala range data fitur. Ini penting dilakukan agar semua fitur memiliki range nilai yang sama sehingga tidak fitur yang mendominasi dengan nilai tinggi sendiri yang dapat mempengaruhi model machine learning
"""

scaler = StandardScaler()

# apply to data
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# reset column and index
X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)
print(f"Scaling data success!")
X_train_scaled.sample(4)

"""## Resampling with Oversampling SMOTE

SMOTE digunakan untuk menambah nilai minoritas agar bernilai sama dengan mayoritas. Oleh karena itu proses ini disebut sebagai Oversampling
"""

smote = SMOTE(random_state=42)

# apply to data
X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)

print(f"Before sample shape: {y_train.value_counts()}\n")
print(f"After shape: {y_train_smote.value_counts()}")

"""## Define evaluate function"""

# define function model evaluation
def model_evaluation(model, X_train, y_train, X_test, y_test):
  # train model
  model.fit(X_train, y_train)

  # predict
  y_test_pred = model.predict(X_test)

  # ROC AUC score
  y_test_proba = model.predict_proba(X_test)[:,1]
  y_train_proba = model.predict_proba(X_train)[:,1]

  train_auc = roc_auc_score(y_train, y_train_proba)
  test_auc = roc_auc_score(y_test, y_test_proba)

  # accuracy score
  test_acc = accuracy_score(y_test, y_test_pred)

  # recall score
  test_recall = recall_score(y_test, y_test_pred)

  # precision score
  test_precision = precision_score(y_test, y_test_pred)

  # f1 score
  test_f1 = f1_score(y_test, y_test_pred)

  # cross validation
  cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
  cv_score = cross_val_score(model, X_train, y_train, cv=cv, scoring='roc_auc')

  # gather all
  model_eval = {
      'Model': model.__class__.__name__,
      'Accuracy test': test_acc,
      'Recall test': test_recall,
      'Precision test': test_precision,
      'F1 test': test_f1,
      'ROC AUC train': train_auc,
      'ROC AUC test': test_auc,
      'Cross Validation': cv_score.mean()
  }

  df_eval = pd.DataFrame(model_eval, index=[0])

  return df_eval



# define function custom confusion matrix
def cust_confusion_matrix(model, X_test, y_test, threshold=0.5):
  y_proba = model.predict_proba(X_test)[:,1]
  y_pred = np.where(y_proba > threshold, 1, 0)

  cm = confusion_matrix(y_test, y_pred)
  cm_percent = cm / cm.sum()

  labels = np.array([['TP','FP'],['FN','TN']])
  labels = np.array([f'{v1}\n{v2} ({percent:.2%})' for v1, v2, percent in zip(labels.flatten(), cm.flatten(), cm_percent.flatten())]).reshape(2,2)

  plt.figure(figsize=(6,6))
  sns.heatmap(cm, annot=labels, fmt='', cmap='Blues',
              annot_kws={'size':'small','weight':'bold'},
              xticklabels=['Good loan','Bad loan'],
              yticklabels=['Good loan','Bad loan'])


  plt.xlabel('Predicted',fontsize='large')
  plt.ylabel('Actual',fontsize='large')
  plt.title('Confusion Matrix')
  plt.show()

# define function roc curve
def cust_roc_curve(model, X_test, y_test):
  # roc auc score
  y_proba = model.predict_proba(X_test)[:,1]
  fpr, tpr, _ = roc_curve(y_test, y_proba)

  auc = roc_auc_score(y_test, y_proba)

  ax.plot(fpr,tpr, label=f"{model.__class__.__name__} AUC = {auc:.2f}")

"""## Training Models"""

models = [
    LogisticRegression(random_state=42),
    RandomForestClassifier(max_depth=6,random_state=42),
    KNeighborsClassifier(),
    XGBClassifier(random_state=42),
    LGBMClassifier(random_state=42)
]

# evaluate models
result = []

for model in models:
  df_eval = model_evaluation(model, X_train_smote, y_train_smote, X_test_scaled, y_test)
  result.append(df_eval)

# display
results_model = pd.concat(result, axis=0).reset_index(drop=True)
results_model

# plot roc curve
fig, ax = plt.subplots(figsize=(10,6))

for model in models:
  cust_roc_curve(model, X_test_scaled, y_test)

ax.plot([0,1], [0,1], 'r--')

ax.set_xlabel('False Positive Rate')
ax.set_ylabel('True Positive Rate')
ax.set_title('ROC Curve')
ax.legend(bbox_to_anchor = (1.02,1), loc='upper left')
plt.show()

"""- `Summary:`
  Berdasarkan tabel evaluasi model, model machine learning dengan performa terbaik yaitu `XGBClassifier`.
- Nilai rata-rata dari masing-masing metrics sekitar **0.90** atau **90%**, nilai ini tergolong sangat tinggi
- Kita bisa menggunakan Hyperparameter tunning untuk memperoleh performa terbaik. Karena nilai metrics model kita sudah jauh lebih baik, kita bisa skip tahapan Hyperparameter tunning.

## Hyperparameter tunning
"""

# you can run this code to get optimal parameters. Because my device has limitation, its take more time than usual.

# # XGBoost model
# model = xgb.XGBClassifier(random_state=42)

# # Hyperparameter grid
# param_grid = {
#     'learning_rate': [0.01, 0.1, 0.2],
#     'max_depth': [3, 5, 7],
#     'n_estimators': [50, 100, 150],
#     'subsample': [0.8, 1.0],
#     'colsample_bytree': [0.8, 1.0],
# }
# # scorer
# roc_auc = make_scorer(roc_auc_score, needs_proba=True)

# # Grid search with cross-validation
# grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring=roc_auc)
# grid_search.fit(X_train_smote, y_train_smote)

# # Best model
# best_model = grid_search.best_estimator_
# print("Best model:", best_model)

# # Best parameters
# print("Best parameters:", grid_search.best_params_)

"""## Top Importance Feature"""

best_model = XGBClassifier(random_state=42)
best_model.fit(X_train_smote, y_train_smote)

feature_importance = best_model.feature_importances_

# get features name
df_importance = pd.DataFrame({'Feature': X_train_smote.columns, 'Importance': feature_importance})
df_importance = df_importance.sort_values(by='Importance', ascending=False)

# plot feature importance
plt.figure(figsize=(10,6))
sns.barplot(x='Importance', y='Feature', data=df_importance)
plt.title('Feature Importance')
plt.show()

"""Terdapat top 4 feature importances:
  
  1. `last_pymnt_amnt`: Fitur ini menjelaskan bahwa jumlah total pembayaran terakhir yang telah diterima dari klien. Fitur ini jelas membantu dalam mengindikasikan Good loan atau Bad loan karena jika total pembayaran yang macet atau tidak sesuai dapat mengkategorikan bahwa klien tersebut merupakan Bad loan.  
  2. `pyment_time`: Fitur ini jumlah bulan klien dari mulai klien diberi dana hingga terakhir klien membayar pinjaman nya. Ini mengindikasikan jika klien tidak tepat waktu maka klien tersebut merupakan Bad loan
  3. `term`: Fitur ini jangka waktu yang diambil klien dalam tenggat membayar seluruh pinjaman nya.
  4. `grade`: Fitur ini membantu LC dalam memberi penilaian atau rating pada Klien dari yang rating tinggi hinggan rating yang buruk.

## Confusion Matrix
"""

# plot confusion matrix
cust_confusion_matrix(best_model, X_test_scaled, y_test)

"""Dalam melihat nilai confusion matrix, biasanya pada model klasifikasi yang perlu diperhatikan yaitu FN (False Negative).

Nilai FN dari hasil model ini yaitu sangat kecil dari keseluruhan data yaitu `0.97%` atau berjumlah `462`.

Metric untuk memperhatikan nilai FN ini yaitu `Recall score`. **Recall score** pada model `XGBClassifier` bernilai 0.95 atau 95% yang mana nilai ini sangat baik.
"""