# -*- coding: utf-8 -*-
"""idx_credit_risk.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UOMCzvXtNEpNPOfqoLhYZxiVfKVe3irZ

# Credit Risk Model Prediction ID/X Partners
Author: Rizki Dinata (June 2024)
"""

from google.colab import drive
drive.mount('/content/drive/')

"""# Import library and dataset"""

import numpy as np
import pandas as pd

pd.set_option('display.max_columns', None)

import matplotlib.pyplot as plt
import seaborn as sns

import warnings
warnings.filterwarnings('ignore')
import os

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_val_score

from sklearn.metrics import roc_curve, accuracy_score, confusion_matrix, classification_report, roc_auc_score, recall_score, precision_score, f1_score
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import OneHotEncoder
from imblearn.over_sampling import SMOTE

from scipy.stats import chi2_contingency, f_oneway

os.chdir('/content/drive/MyDrive/Projects/credit-risk-idx')

!pwd

#load data
df = pd.read_csv('idx_credit.csv')
df.head()

#display total rows and columns
df.shape

df.info()

#split categorical and numerical data
cat_df = df.select_dtypes(include='object')
num_df = df.select_dtypes(exclude='object')

cats_col = cat_df.columns.to_list()
nums_col = num_df.columns.to_list()

df[nums_col].describe()

#display count unique value categorical data
for col in cats_col:
  print(df[col].value_counts())
  print('---'*30)

"""# Data Preprocessing"""

df_clean = df.copy()

"""## Drop Columns with Missing Values"""

#check missing value with percentage and add style background

def missing_percentage(df):
  missing_values = df.isnull().sum()
  total_values = len(df)
  missing_percentage = (missing_values / total_values) * 100
  missing_df = pd.DataFrame({'column_name': df.columns,
                             'missing_values': missing_values,
                             'missing_percentage': missing_percentage
                              })
  missing_df = missing_df.sort_values('missing_percentage', ascending=False).reset_index(drop=True)
  missing_df['missing_percentage'] = missing_df['missing_percentage'].apply('{:.2f}'.format)
  return missing_df

result_missing = missing_percentage(df_clean)
result_missing.style.background_gradient(cmap='Reds', subset=['missing_percentage'])

#drop feature or columns with missing value percentage more than 50

df_clean = df_clean.drop(columns=result_missing[result_missing['missing_percentage'].astype('float') > 50]['column_name'])

df_clean.shape

"""## Impute Missing Values under 50%"""

# define impute missing value if number input median value and if its object impute mode

def impute_missing_values(df):
  for col in df.columns:
    if df[col].dtype == 'number':
      df[col] = df[col].fillna(df[col].median())
    else:
      df[col] = df[col].fillna(df[col].mode()[0])
  return df

df_clean = impute_missing_values(df_clean)

df_clean.info()

"""## Check Duplicate data"""

df_clean.duplicated().sum()

"""## Check unique values"""

print("Unique Values Numerical: \n")
df_clean.select_dtypes(include='number').nunique()

"""* `Unnamed: 0 `,`id`, `member_id` contain unique values
* But `policy_code` contains 1 value
"""

print("Unique Values Categorical: \n")
df_clean.select_dtypes(include='object').nunique()

"""* `emp_title`, `url`, `title` have unique values up to 1000, this values too much for categorical feature for train machine learning
* `application_type` has 1 value
* `issue_d`, `last_pymnt_d`, `next_pymnt_d`, `last_credit_pull_d` have to **datetime** format. So, this feature would be transformed
"""

df_clean['term'].unique()

"""* delete whitespace"""

# delete whitespcae unique value in feature term

df_clean['term'] = df_clean['term'].apply(lambda x: x.strip())
df_clean['term'].unique()

df_clean['grade'].unique()

df_clean['sub_grade'].unique()

df_clean['emp_length'].unique()

df_clean['home_ownership'].unique()

df_clean['verification_status'].unique()

df_clean['pymnt_plan'].unique()

"""## Define target variable

* `loan_status` is label of this dataset.
* We need to reclassification this label from 9 classification to binary classification
"""

df_clean['loan_status'].unique()

df_clean['loan_status'].value_counts()

"""* Bad loan (1) : `Charged Off`, `Default`, `Late (31-120 days)`, `Does not meet the credit policy. Status:Charged Off`, `Late (16-30 days)`
* Good loan (0) : `Fully Paid`, `Does not meet the credit policy. Status:Fully Paid`
* Uncertained loan (-1) : `Current`, `In Grace Period`
* We will use Good loan (1) and Bad loan (0) as our classification target
* Uncertained loan (-1) will be deleted
"""

# Define dictionary for encoding target variable
target = {'Fully Paid':'Good loan',
          'Does not meet the credit policy. Status:Fully Paid':'Good loan',
          'Charged Off':'Bad loan',
          'Default':'Bad loan',
          'Late (31-120 days)':'Bad loan',
          'Does not meet the credit policy. Status:Charged Off':'Bad loan',
          'Late (16-30 days)':'Bad loan',
          'Current':'Uncertained loan',
          'In Grace Period':'Uncertained loan'}
# Mapping values in target column
df_clean['loan_status'] = df_clean['loan_status'].map(target)

df_clean['loan_status'].value_counts()

# drop loan status having value -1

df_clean = df_clean[df_clean['loan_status'] != 'Uncertained loan'].reset_index()
df_clean['loan_status'].value_counts()

df_clean.shape

"""## Reformatting Datetime columns"""

df_clean['earliest_cr_line'].value_counts()

def date(dt):
  if dt.year > 2016:
    dt = dt.replace(year=dt.year - 100)
  return dt

# The month which the loan was funded
df_clean['issue_d'] = pd.to_datetime(df_clean['issue_d'], format='%b-%y')
df_clean['last_pymnt_d'] = pd.to_datetime(df_clean['last_pymnt_d'], format='%b-%y')
df_clean['last_credit_pull_d'] = pd.to_datetime(df_clean['last_credit_pull_d'], format='%b-%y')
df_clean['earliest_cr_line'] = pd.to_datetime(df_clean['next_pymnt_d'], format='%b-%y')
df_clean['earliest_cr_line'] = df_clean['earliest_cr_line'].apply(lambda x: date(x))

# display result
df_clean[['issue_d', 'last_pymnt_d', 'last_credit_pull_d', 'earliest_cr_line']].tail()

"""## Create new features

* `credit_pull_year` = total years between `earliest_cr_line` and `last_credit_pull_d`
* `pyment_time` = total months between `issue_d` and `last_pymnt_d`
"""

# define function to create new features
def total_month(dt1, dt2):
  return (dt2.year - dt1.year) * 12 + (dt2.month - dt1.month)

def total_year(dt1, dt2):
  return dt2.year - dt1.year

df_clean['credit_pull_year'] = df_clean.apply(lambda x: total_year(x['last_credit_pull_d'], x['earliest_cr_line']), axis=1)
df_clean['pyment_time'] = df_clean.apply(lambda x: total_month(x['issue_d'], x['last_pymnt_d']), axis=1)

# display result
df_clean[['credit_pull_year', 'pyment_time']].head()

df_clean.info()

df_clean.head()

"""## Save Cleaned Data"""

# save cleaned data
df_clean.to_csv('idx_credit_clean.csv', index=False)
print('Saving data is done!!')

"""**This data will be used for Exploratory Data Analysis (EDA) to find valuable insight**

# Exploratory Data Analysis (EDA)

## Target Variable
Loan status is the target variable in this process.
"""

# visualize loan status before transforming to binary classification
orders = df['loan_status'].value_counts().index

colors = ["dodgerblue" if stats in ['Fully Paid', 'Does not meet the credit policy. Status:Fully Paid'] else
          "red" if stats in ['Charged Off', 'Default', 'Late (31-120 days)', 'Does not meet the credit policy. Status:Charged Off', 'Late (16-30 days)'] else
          "gray" for stats in orders]

plt.figure(figsize=(10, 6))

sns.countplot(y='loan_status', order=orders, palette=colors,  data=df)
plt.title('Loan Status Distribution', fontsize=16)
plt.xlabel('Count', fontsize=12)
plt.ylabel('Loan Status', fontsize=12)


plt.show()

"""Current and In Grace Period will be dropped because it cannot be determined as Good loan or Bad loan

## Load cleaned data from storage
"""

# load cleaned data
df_clean = pd.read_csv('idx_credit_clean.csv')
df_clean.head()

# Calculate profit and loss for good and bad loans
good_loans = df_clean[df_clean['loan_status'] == 'Good loan']
bad_loans = df_clean[df_clean['loan_status'] == 'Bad loan']

good_loan_profit = good_loans['total_pymnt'].sum() - good_loans['total_rec_prncp'].sum()
bad_loan_loss = bad_loans['total_rec_prncp'].sum() - bad_loans['total_pymnt'].sum()

print(f"Profit from good loans: {good_loan_profit}")
print(f"Loss from bad loans: {bad_loan_loss} \n")

# create pie chart
plt.figure(figsize=(15, 4))

plt.subplot(1,2,1)
df_clean['loan_status'].value_counts().plot(kind='pie',
                                            autopct='%1.1f%%',
                                            startangle=90,
                                            explode=[0,0.05],
                                            colors=['dodgerblue','red'])

# create bar plot
plt.subplot(1,2,2)
sns.countplot(x='loan_status', data=df_clean, palette=['dodgerblue','red'])

plt.show()

"""* Based on this graph, Loan status feature contains imbalanced data that **78.2%** Good loan and **21.8%** Bad loan.
* The profit of Good loan is **382,698,243** and The loss of Bad loan is **-166,701,734**

## Plotting Function
"""

# 1. Define Barplot
def barplot_cust(data, x, y, palette=None, hue=None, order=None):
  plt.figure(figsize=(10, 6))
  sns.barplot(x=x, y=y, data=data, palette=palette, hue=hue, order=order)

  plt.title(f"{hue.replace('_',' ').title()} {y.replace('_',' ').title()} by {x.replace('_',' ').title()}")
  plt.xlabel(f"{x.replace('_',' ').title()}")
  plt.ylabel(f"{y.replace('_',' ').title()}")
  plt.legend(f"{hue.replace('_',' ').title()}")


# 2. Define Line plot
def lineplot_cust(data, x, y, hue=None, palette=None, ci=95):
  sns.lineplot(x=x, y=y, data=data, hue=hue, palette=palette, ci=ci)

  plt.title(f"{hue.replace('_',' ').title()} {y.replace('_',' ').title()} by {x.replace('_',' ').title()}")
  plt.xlabel(f"{x.replace('_',' ').title()}")
  plt.ylabel(f"{y.replace('_',' ').title()}")
  plt.legend(title=f"{hue.replace('_',' ').title()}", loc='best')

"""## Loan over years"""

# change issue_d to datetime
df_clean['issue_d'] = pd.to_datetime(df_clean['issue_d'])

# create new column year
df_clean['year'] = df_clean['issue_d'].dt.year

plt.figure(figsize=(18, 8))

# Funded amnt by years
plt.subplot(2,2,1)
lineplot_cust(x='year', y='funded_amnt', data=df_clean, hue='loan_status', palette=['dodgerblue','red'])
plt.xlabel(None)

# Debt to income ratio by years
plt.subplot(2,2,3)
lineplot_cust(x='year', y='dti', data=df_clean, hue='loan_status', palette=['dodgerblue','red'])
plt.xlabel(None)

# annual income by years
plt.subplot(2,2,2)
lineplot_cust(x='year', y='annual_inc', data=df_clean, hue='loan_status', palette=['dodgerblue','red'])
plt.xlabel(None)

# Revolving balance by years
plt.subplot(2,2,4)
lineplot_cust(x='year', y='revol_bal', data=df_clean, hue='loan_status', palette=['dodgerblue','red'])
plt.xlabel(None)

plt.tight_layout()
plt.show()

"""* Funded Amount over Years

  - The total loans given have increased year over year.

  - The total loans given to Good loans are less than those given to Bad loans.

  - From 2009 to 2010, the loans given to Bad loans decreased but increased again in the following year.

* Annual Income over Years

  - This feature explains the annual income reported directly by clients.

  - An interesting point is that the income for Good loans was lower than for Bad loans in 2007-2008. However, the income for Good loans continued to increase in the following years.

  - While the income for Bad loans was higher in 2007-2008 than for Good loans, it continued to decrease and had a difference of up to 10,000 in the latest data.

* Debt to Income (DTI) over Years

  - DTI is the ratio of debt to income.

  - DTI has continuously increased year over year.

  - DTI for Bad loans is higher than for Good loans.

  - This data explains that the income of clients has decreased, and the amount of loans given to Bad loan clients has continued to increase. However, according to some sources, this DTI is still tolerable as it is below 36.

* Revolving Balance over Years

  - There was a significant spike in the Revolving Balance for Bad loans in 2008, which was also higher than for Good loans by around 10,000. This also explains why the annual income for Bad loans was still high in the same year.

  - In the following years, the Revolving Balance for Bad loans continued to decline, following the trend of the Revolving Balance for Good loans.

  - This indicates that clients are trying to reduce their total Revolving Balance due to decreasing income, leading to financial instability.

- From this data, we can see that Bad loan clients experienced a decrease in income but their needs continued to rise. Consequently, Bad loan clients want to slowly reduce their total debt, especially after the ballooning debt in 2008.

- Meanwhile, Good loan clients, although their income continues to rise, adjust their Revolving Balance in line with their income. This is evident from their DTI, which does not show spikes despite the continuous increasing trend. The increasing trend in DTI almost follows the income trend of Good loan clients themselves.

## Loan status by purpose
"""

# define function custom barplot
def barplot_cust(df, y, palette=None, num_top=5):
    cnt = df[y].value_counts()

    # Plot the countplot
    num_top = num_top
    palette = ['dodgerblue']*num_top + ['gray']*(len(cnt) - num_top)

    ax = sns.countplot(y=y, data=df, order=cnt.index, palette=palette)

    # Add the count labels to the bars

    for p in ax.patches:
        # Calculate label position
        width = p.get_width()
        x = width + 0.1  # Adjust offset as needed

        # Add the label
        ax.text(x, p.get_y() + p.get_height() / 2,
                '{:1.0f}'.format(width),
                ha="left", va="center", fontsize='small')
    plt.title(f"Distribution of {y.title()}")
    plt.xlabel('Count')
    plt.ylabel(y.title())


# define function stacked plot
def stacked_barplot(df, x, y, palette=None, order=None):
    # grouping by variable and loan status
    var = df.groupby(x)[y].value_counts().unstack()
    var = var.reindex(order)

    # calculate percentage
    var_pct = var.div(var.sum(axis=1), axis=0).mul(100)

    # Determine the order of the bars for stacking
    bar1 = plt.bar(var_pct.index,
                   var_pct['Good loan'],
                   label='Good loan',
                   color=palette[0])
    bar2 = plt.bar(var_pct.index,
                   var_pct['Bad loan'],
                   label='Bad loan',
                   color=palette[1],
                   bottom=var_pct['Good loan'])

    # Set the order of the stacked bars
    stack_order = ['Bad loan', 'Good loan']

    # Add labels to each bar segment
    for bar1, bar2 in zip(bar1, bar2):
        height1 = bar1.get_height()
        height2 = bar2.get_height()

        plt.text(bar1.get_x() + bar1.get_width() / 2,
                 height1 / 2,
                 '{:.1f}%'.format(height1),
                 ha='center',
                 va='center')

        plt.text(bar2.get_x() + bar2.get_width() / 2,
                 height1 + height2 / 2,
                 '{:.1f}%'.format(height2),
                 ha='center',
                 va='center')

    # Add label and set the order of the stacked bars
    plt.title(f"Default Rate by {x.replace('_', ' ').title()}", fontsize='large')
    plt.xlabel(f"{x.replace('_', ' ').title()}")
    plt.ylabel('Total Clients (%)')
    plt.legend(title=y.replace('_', ' ').title(), loc='best', bbox_to_anchor=(1.01, 1))

# select top 5 purpose of loan
top_val = df_clean['purpose'].value_counts().head(5).index
top_purpose = df_clean[df_clean['purpose'].isin(top_val)]
order_purpose = top_purpose['purpose'].value_counts().index

# create figure
plt.figure(figsize=(18, 6))

# plot all purpose
plt.subplot(1,2,1)
barplot_cust(df=df_clean, y='purpose')

# plot percentage top 5 purpose
plt.subplot(1,2,2)
stacked_barplot(df=top_purpose, x='purpose', y='loan_status', order=order_purpose, palette=['dodgerblue','red'])

plt.tight_layout()
plt.show()

"""* Insight
  - There are 5 interests that clients make loans for, namely debt consolidation, credit card, home improvement, other, and major purchase.
  - Major purchase has a high Good loan rate of 83.9%. While the high Bad loan rate is Other (other interests).
  - Because we cannot determine the interests specifically, so we choose the highest Bad loan rate after Other, namely Debt consolidation, which is 22.9%.
  - So Debt Consolidation deserves to be our point of attention because Debt consolidation is also the most client interest in making loans, which is around 139,000.

## Loan status by state
"""

# select top 5 states
top_state = df_clean['addr_state'].value_counts().head(5).index
top_state_df = df_clean[df_clean['addr_state'].isin(top_state)]
order_state = top_state_df['addr_state'].value_counts().index

plt.figure(figsize=(18, 6))

# plot by addr state
plt.subplot(1,2,1)
barplot_cust(df=top_state_df, y='addr_state')

# plot percentage by addr state
plt.subplot(1,2,2)
stacked_barplot(df=top_state_df, x='addr_state', y='loan_status', order=order_state, palette=['dodgerblue','red'])

plt.tight_layout()
plt.show()

"""* Insight
  - The top 5 states where clients are located are CA (California), NY (New York), TX (Texas), FL (Florida), and NJ (New Jersey).
  - The highest number of clients from California is **40550**.
  - The highest Good loan rate is clients from TX (Texas) which is **80.2%**
  - While the highest Bad loan rate is clients from FL (Florida) which is **24.3%**

## Loan status by verification status
"""

# loan status by verification status


plt.figure(figsize=(18, 6))

plt.subplot(1,2,1)
barplot_cust(df=df_clean, y='verification_status')

plt.subplot(1,2,2)
stacked_barplot(df=df_clean, x='verification_status', y='loan_status', palette=['dodgerblue','red'])

"""* Insight
 - Clients with verified income have the highest number of **88,823**.
  - But clients with verified income also have the highest bad loan rate of **24.6%**.
  - Meanwhile, clients with unverified income have the lowest bad loan rate of **17.1%**.

## Loan status by Grade
"""

# loan status by grade
plt.figure(figsize=(18, 6))

plt.subplot(1,2,1)
barplot_cust(df=df_clean, y='grade')

plt.subplot(1,2,2)
stacked_barplot(df=df_clean, x='grade', y='loan_status', palette=['dodgerblue','red'])

"""* Insight
  - Clients with Grade B have the highest number at **72,239**.
  - The highest Bad loan rate is Grade G around **47.5%**
  - While the lowest Bad loan rate is Grade A around **7.5%**.
  - This explains that the higher the client's Grade level, the lower the Bad loan rate.

## Loan status by employee length
"""

# loan status by employee length
plt.figure(figsize=(25, 6))

plt.subplot(1,2,1)
barplot_cust(df=df_clean, y='emp_length')

plt.subplot(1,2,2)
stacked_barplot(df=df_clean, x='emp_length', y='loan_status', palette=['dodgerblue','red'])

"""* Insight
 - Clients with a length of service of 10 years and above have the largest number of clients, namely 80,537.
  - While the Bad loan rate is not much different around 21%

## Loan status by term
"""

# loan status by term
plt.figure(figsize=(20,4))

plt.subplot(1,2,1)
barplot_cust(df=df_clean, y='term')

plt.subplot(1,2,2)
stacked_barplot(df=df_clean, x='term', y='loan_status', palette=['dodgerblue','red'])

"""* Insight
  - The 36 months term has the highest number of clients at 186,469 or more than 60% of clients choose the 36 months (3 years) term.
  - The highest Bad loan rate is with a term of 60 months around 35.5%.
  - This is enough to explain that the longer the term specified, the higher the Bad loan rate.
"""

# select top 5 emp title
top_state = df_clean['emp_title'].value_counts().head(5).index
top_state_df = df_clean[df_clean['emp_title'].isin(top_state)]
order_state = top_state_df['emp_title'].value_counts().index

plt.figure(figsize=(18, 6))

# plot by addr state
plt.subplot(1,2,1)
barplot_cust(df=df_clean, y='emp_title')

# plot percentage by addr state
plt.subplot(1,2,2)
stacked_barplot(df=top_state_df, x='emp_title', y='loan_status', order=order_state, palette=['dodgerblue','red'])

plt.tight_layout()
plt.show()

"""# Drop unnecessary Columns"""

un_col = [
    'index',
    'id',
    'member_id',
    'Unnamed: 0',
    'url',
    'title',
    'zip_code',
    'addr_state',
    'sub_grade',
    'emp_title',
    'pymnt_plan',
    'collections_12_mths_ex_med',
    'policy_code',
    'application_type',
    'out_prncp',
    'out_prncp_inv',
    'total_rec_prncp',
    'total_rec_late_fee',
    'recoveries',
    'collection_recovery_fee'
    ]

col_date = [
    'issue_d',
    'last_pymnt_d',
    'last_credit_pull_d',
    'earliest_cr_line',
    'next_pymnt_d'
    ]

remove_col = un_col + col_date

df_cl = df_clean.copy()
df_cl = df_cl.drop(columns=remove_col, axis=1)

"""* The features `url`, `title`, `addr_state`, `sub_grade`, `emp_title` will be dropped along with the columns that are not required for modeling due to the large number of categories.
* The datetime feature will also be dropped due to the small number of unique values.

# Train Test Split
"""

X = df_cl.drop(columns=['loan_status'])
y = df_cl[['loan_status']]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"y_test shape: {y_test.shape}")

"""# Feature Engineering

## Encoding Data
One Hot Encoding is a method to convert categorical columns into binary to make machine learning models easier to analyze and predict. One Hot Encoding is an encodde library that has been provided by Scikit-learn.\
In addition, encoding can also be done manually by creating an encoding target dictionary.
"""

# defince function encoding data
def encoding_x(df):
  # Map grade
  grade_dict = {'A':6,
                'B':5,
                'C':4,
                'D':3,
                'E':2,
                'F':1,
                'G':0}
  df['grade'] = df['grade'].map(grade_dict)

  # map emp_length
  emp_length_dict = {'< 1 year':0,
                    '1 year':1,
                    '2 years':2,
                    '3 years':3,
                    '4 years':4,
                    '5 years':5,
                    '6 years':6,
                    '7 years':7,
                    '8 years':8,
                    '9 years':9,
                    '10+ years':10}
  df['emp_length'] = df['emp_length'].map(emp_length_dict)

  # map term
  term_dict = {'36 months':36,
               '60 months':60}
  df['term'] = df['term'].map(term_dict)

  # map home ownership and encoding with OHE -----------
  home_dict = {'MORTGAGE':'MORTGAGE',
               'RENT':'RENT',
               'OWN':'OWN',
               'OTHER':'OTHER',
               'ANY':'OTHER',
               'NONE':'OTHER'}
  df['home_ownership'] = df['home_ownership'].map(home_dict)

  # aggregate variable purpose and encoding eith OHE --------------
  purpose_dict = {'debt_consolidation':'debt_consolidation',
               'credit_card':'credit_card',
               'home_improvement':'private_use',
               'other':'other',
               'major_purchase':'major_purchase',
               'small_business':'small_business',
               'car':'private_use',
               'medical':'private_use',
               'wedding':'private_use',
               'moving':'private_use',
               'house':'private_use',
               'vacation':'private_use',
               'educational':'private_use',
               'renewable_energy':'other'}
  df['purpose'] = df['purpose'].map(purpose_dict)

  # Encoding with OHE
  var_ohe = ['home_ownership', 'verification_status', 'purpose', 'initial_list_status']
  encoder = OneHotEncoder(sparse=False,handle_unknown='ignore')

  for var in var_ohe:
    # Fit and transform columns
    encoded_var = encoder.fit_transform(df[[var]])

    # Get new columns
    encoded_var_columns = encoder.get_feature_names_out([var])

    # Create new dataframe encoded columns
    encoded_df = pd.DataFrame(encoded_var, columns=encoded_var_columns, index=df.index)

    # merge real data with dataframe encoded
    df = pd.concat([df, encoded_df], axis=1)

    # Drop columns after encoded
    df.drop(var, axis=1, inplace=True)
  print('Encoding features success!')
  return df

def encoding_y(df):
    # map target
    lstatus = {'Good loan':0, 'Bad loan':1}
    df['loan_status'] = df['loan_status'].map(lstatus)

    print('Encoding target success!')
    return df

# Apply to data train
X_train = encoding_x(X_train)
y_train = encoding_y(y_train)

# Apply to data test
X_test = encoding_x(X_test)
y_test = encoding_y(y_test)

"""# Feature Selection

## Pearson test
"""

# correlation matrix
var_corr = X_train.corr()

plt.figure(figsize=(30,13))
sns.heatmap(var_corr, annot=True, cmap='coolwarm')
plt.show()

# drop multicolinearity columns
multi_cols = ['loan_amnt','funded_amnt_inv','installment','total_pymnt','total_pymnt_inv','total_rec_int','int_rate','total_rev_hi_lim']

# apply to data
X_train = X_train.drop(columns=multi_cols, axis=1)
X_test = X_test.drop(columns=multi_cols, axis=1)

"""Drop the multicollinearity column to prevent model error in predicting the coefficient of the relationship between the independent variable and the dependent variable.

## Detect outliers
"""

def detect_outliers(df, cols):
  for col in cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1

    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # outlier check
    df[col] = np.where((df[col] < lower_bound) | (df[col] > upper_bound), np.clip(df[col],lower_bound,upper_bound), df[col])

  return df

def box_outliers(df, cols, width=18, height=10):
  rows = (len(cols) - 1) // 9 + 1
  plt.figure(figsize=(width, height))

  for i, col in enumerate(cols, 1):
    plt.subplot(rows, 9, i)
    sns.boxplot(df[col], orient='v')
    plt.ylabel(None)
    plt.title(f"{col.title()}", fontsize='medium')
    plt.tight_layout()

  plt.show()


def outliers_kde(df, cols, width=18, height=10):
    num_rows = (len(cols) - 1) // 9 + 1

    plt.figure(figsize=(width, height))

    for i, col in enumerate(cols,1):
        plt.subplot(num_rows, 9, i)
        sns.kdeplot(df[col], fill=True, alpha=0.7)
        plt.title(col, fontsize="medium")
        plt.xlabel(None)
        plt.ylabel(None)
        plt.tight_layout()

    plt.show()

num_cols = X_train.select_dtypes(include='number').columns

# check box outliers
box_outliers(X_train, num_cols)

# check with kde
outliers_kde(X_train, num_cols)

# focus remove outliers on the features not new features after encoding
cols_target = ['annual_inc','delinq_2yrs','inq_last_6mths','open_acc','pub_rec','revol_bal','revol_util','tot_cur_bal','credit_pull_year','pyment_time','last_pymnt_amnt','acc_now_delinq','tot_coll_amt','total_acc']


X_train = detect_outliers(X_train, cols_target)

# check box outliers
box_outliers(X_train, num_cols)

"""Drop features contain zero value"""

cols_drop = ['pub_rec', 'delinq_2yrs','acc_now_delinq', 'tot_coll_amt']

X_train = X_train.drop(columns=cols_drop, axis=1)
X_test = X_test.drop(columns=cols_drop, axis=1)

"""# Modelling

## Scaling Data (Standard scaler)
Standard scaler is used to transform the scale of the feature data range. This is important so that all features have the same range of values so that no feature dominates with its own high values that can affect the machine learning model.
"""

scaler = StandardScaler()

# apply to data
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# reset column and index
X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)
print(f"Scaling data success!")
X_train_scaled.sample(4)

"""## Resampling with Oversampling SMOTE

SMOTE is used to increase the value of the minority to make it equal to the majority. Hence this process is referred to as Oversampling
"""

smote = SMOTE(random_state=42)

# apply to data
X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)

print(f"Before sample shape: {y_train.value_counts()}\n")
print(f"After shape: {y_train_smote.value_counts()}")

"""## Define evaluate function"""

# define function model evaluation
def model_evaluation(model, X_train, y_train, X_test, y_test):
  # train model
  model.fit(X_train, y_train)

  # predict
  y_test_pred = model.predict(X_test)

  # ROC AUC score
  y_test_proba = model.predict_proba(X_test)[:,1]
  y_train_proba = model.predict_proba(X_train)[:,1]

  train_auc = roc_auc_score(y_train, y_train_proba)
  test_auc = roc_auc_score(y_test, y_test_proba)

  # accuracy score
  test_acc = accuracy_score(y_test, y_test_pred)

  # recall score
  test_recall = recall_score(y_test, y_test_pred)

  # precision score
  test_precision = precision_score(y_test, y_test_pred)

  # f1 score
  test_f1 = f1_score(y_test, y_test_pred)

  # cross validation
  cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
  cv_score = cross_val_score(model, X_train, y_train, cv=cv, scoring='roc_auc')

  # gather all
  model_eval = {
      'Model': model.__class__.__name__,
      'Accuracy test': test_acc,
      'Recall test': test_recall,
      'Precision test': test_precision,
      'F1 test': test_f1,
      'ROC AUC train': train_auc,
      'ROC AUC test': test_auc,
      'Cross Validation': cv_score.mean()
  }

  df_eval = pd.DataFrame(model_eval, index=[0])

  return df_eval



# define function custom confusion matrix
def cust_confusion_matrix(model, X_test, y_test, threshold=0.5):
  y_proba = model.predict_proba(X_test)[:,1]
  y_pred = np.where(y_proba > threshold, 1, 0)

  cm = confusion_matrix(y_test, y_pred)
  cm_percent = cm / cm.sum()

  labels = np.array([['TP','FP'],['FN','TN']])
  labels = np.array([f'{v1}\n{v2} ({percent:.2%})' for v1, v2, percent in zip(labels.flatten(), cm.flatten(), cm_percent.flatten())]).reshape(2,2)

  plt.figure(figsize=(6,6))
  sns.heatmap(cm, annot=labels, fmt='', cmap='Blues',
              annot_kws={'size':'small','weight':'bold'},
              xticklabels=['Good loan','Bad loan'],
              yticklabels=['Good loan','Bad loan'])


  plt.xlabel('Predicted',fontsize='large')
  plt.ylabel('Actual',fontsize='large')
  plt.title('Confusion Matrix')
  plt.show()

# define function roc curve
def cust_roc_curve(model, X_test, y_test):
  # roc auc score
  y_proba = model.predict_proba(X_test)[:,1]
  fpr, tpr, _ = roc_curve(y_test, y_proba)

  auc = roc_auc_score(y_test, y_proba)

  ax.plot(fpr,tpr, label=f"{model.__class__.__name__} AUC = {auc:.2f}")

"""## Training Models"""

models = [
    LogisticRegression(random_state=42),
    RandomForestClassifier(max_depth=6,random_state=42),
    KNeighborsClassifier(),
    XGBClassifier(random_state=42),
    LGBMClassifier(random_state=42)
]

# evaluate models
result = []

for model in models:
  df_eval = model_evaluation(model, X_train_smote, y_train_smote, X_test_scaled, y_test)
  result.append(df_eval)

# display
results_model = pd.concat(result, axis=0).reset_index(drop=True)
results_model

# plot roc curve
fig, ax = plt.subplots(figsize=(10,6))

for model in models:
  cust_roc_curve(model, X_test_scaled, y_test)

ax.plot([0,1], [0,1], 'r--')

ax.set_xlabel('False Positive Rate')
ax.set_ylabel('True Positive Rate')
ax.set_title('ROC Curve')
ax.legend(bbox_to_anchor = (1.02,1), loc='upper left')
plt.show()

"""`Summary:`
- Based on the model evaluation table, the best performing machine learning model is `XGBClassifier`.
- The average value of each metrics is about **0.90** or **90%**, which is very high.
- We can use Hyperparameter tunning to get the best performance. Since the metrics value of our model is already much better, we can skip the Hyperparameter tunning step.

## Hyperparameter tunning
"""

# you can run this code to get optimal parameters. Because my device has limitation, its take more time than usual.

# # XGBoost model
# model = xgb.XGBClassifier(random_state=42)

# # Hyperparameter grid
# param_grid = {
#     'learning_rate': [0.01, 0.1, 0.2],
#     'max_depth': [3, 5, 7],
#     'n_estimators': [50, 100, 150],
#     'subsample': [0.8, 1.0],
#     'colsample_bytree': [0.8, 1.0],
# }
# # scorer
# roc_auc = make_scorer(roc_auc_score, needs_proba=True)

# # Grid search with cross-validation
# grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring=roc_auc)
# grid_search.fit(X_train_smote, y_train_smote)

# # Best model
# best_model = grid_search.best_estimator_
# print("Best model:", best_model)

# # Best parameters
# print("Best parameters:", grid_search.best_params_)

"""## Top Importance Feature"""

best_model = XGBClassifier(random_state=42)
best_model.fit(X_train_smote, y_train_smote)

feature_importance = best_model.feature_importances_

# get features name
df_importance = pd.DataFrame({'Feature': X_train_smote.columns, 'Importance': feature_importance})
df_importance = df_importance.sort_values(by='Importance', ascending=False)

# plot feature importance
plt.figure(figsize=(10,6))
sns.barplot(x='Importance', y='Feature', data=df_importance)
plt.title('Feature Importance')
plt.show()

"""There are top 4 feature importances:
  
  1. `last_pymnt_amnt`: This feature explains that the total amount of the last payment that has been received from the client. This feature clearly helps in indicating a Good loan or Bad loan because if the total payment is stuck or not appropriate it can categorize that client as a Bad loan.  
  2. `pyment_time`: This feature is the number of months from when the client was given funds to when the client last paid the loan. This indicates if the client is not on time then the client is a Bad loan.
  3. `term`: This feature is the period of time that the client takes to repay the entire loan.
  4. `grade`: This feature assists LC in rating the Client from a high rating to a poor rating.

## Confusion Matrix
"""

# plot confusion matrix
cust_confusion_matrix(best_model, X_test_scaled, y_test)

"""In looking at the confusion matrix value, usually the classification model that needs to be considered is FN (False Negative).

The FN value of the results of this model is very small from the entire data, namely `0.97%` or totaling `462`.

The metric to pay attention to this FN value is `Recall score`. The **Recall score** of the `XGBClassifier` model is 0.95 or 95% which is very good.
"""